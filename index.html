<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="ELF-P">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Efficient Learning of High Level Plans from Play</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-72PW1FZDE4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-72PW1FZDE4');
  </script>
<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<title>appendix-experiments</title>
<style>
  html {
    line-height: 1.5;
    font-family: Georgia, serif;
    font-size: 20px;
    color: #1a1a1a;
    background-color: #fdfdfd;
  }
  body {
    margin: 0 auto;
    max-width: 100em;
    padding-left: 50px;
    padding-right: 50px;
    padding-top: 50px;
    padding-bottom: 50px;
    hyphens: auto;
    overflow-wrap: break-word;
    text-rendering: optimizeLegibility;
    font-kerning: normal;
  }


  figcaption {
    padding: 1px;
    font-size: 15px;
    caption-side: bottom;
    text-align: center;

}
figure {
  margin: 2rem 0;
  position: relatve;
  display: inline-box;
}

figure img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.column3 {
  float: left;
  width: 33.33%;
  padding: 5px;
}
.column2 {
  float: left;
  width: 50%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}

  @media (max-width: 600px) {
    body {
      font-size: 0.9em;
      padding: 1em;
    }
    h1 {
      font-size: 1.8em;
    }
  }
  @media print {
    body {
      background-color: transparent;
      color: black;
      font-size: 12pt;
    }
    p, h2, h3 {
      orphans: 3;
      widows: 3;
    }
    h2, h3, h4 {
      page-break-after: avoid;
    }
  }
  p {
    margin: 1em 0;
  }
  a {
    color: #1a1a1a;
  }
  a:visited {
    color: #1a1a1a;
  }
  img {
    max-width: 100%;
  }
  h1, h2, h3, h4, h5, h6 {
    margin-top: 1.4em;
  }
  h5, h6 {
    font-size: 1em;
    font-style: italic;
  }
  h6 {
    font-weight: normal;
  }
  ol {
    padding-left: 8em;
    margin-top: 1em;
    columns: 2;
  }

  ul {
    padding-left: 1.5em;
    margin-top: 1em;
  }
  li > ol, li > ul {
    margin-top: 0;
  }
  blockquote {
    margin: 1em 0 1em 1.7em;
    padding-left: 1em;
    border-left: 2px solid #e6e6e6;
    color: #606060;
  }
  code {
    font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
    font-size: 85%;
    margin: 0;
  }
  pre {
    margin: 1em 0;
    overflow: auto;
  }
  pre code {
    padding: 0;
    overflow: visible;
    overflow-wrap: normal;
  }
  .sourceCode {
   background-color: transparent;
   overflow: visible;
  }
  hr {
    background-color: #1a1a1a;
    border: none;
    height: 1px;
    margin: 1em 0;
  }
  table {
    margin: 1em 0;
    border-collapse: collapse;
    width: 100%;
    overflow-x: auto;
    display: block;
    font-variant-numeric: lining-nums tabular-nums;
  }
  caption {
    margin-bottom: 0.75em;
    font-size: 15px;
    caption-side:bottom;
    text-align: left;
  }
  tbody {
    margin-top: 0.5em;
    border-top: 1px solid #1a1a1a;
    border-bottom: 1px solid #1a1a1a;
  }
  th {
    border-top: 1px solid #1a1a1a;
    padding: 0.25em 0.5em 0.25em 0.5em;
  }
  td {
    padding: 0.125em 0.5em 0.25em 0.5em;
  }
  header {
    margin-bottom: 4em;
    text-align: center;
  }
  #TOC li {
    list-style: none;
  }
  #TOC ul {
    padding-left: 1.3em;
  }
  #TOC > ul {
    padding-left: 0;
  }
  #TOC a:not(:hover) {
    text-decoration: none;
  }
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  div.columns{display: flex; gap: min(4vw, 1.5em);}
  div.column{flex: auto; overflow-x: auto;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
  ul.task-list li input[type="checkbox"] {
    width: 0.8em;
    margin: 0 0.8em 0.2em -1.6em;
    vertical-align: middle;
  }
</style>
<script
var alg = new String("Welcome to Tutorials Point");

src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
type="text/javascript"></script>
<!--[if lt IE 9]>
  <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
<![endif]-->
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Efficient Learning of High Level Plans from Play</h1>
   
          <div class="is-size-5 publication-authors">
            <span class="author-block">  <b>Accepted to the International Conference on Robotics and Automation (ICRA) 2023. This page contains supplementary materials.</b></span>
          </div>
      </div>
    </div>
</section>
<h3 class="title is-5">A.1.1 Training environment</h3>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract.  -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Real-world robotic manipulation tasks remain an elusive challenge, since they involve both fine-grained environment interaction, as well as the ability to plan for long-horizon goals. Although deep reinforcement learning (RL) methods have shown encouraging results when planning end-to-end in high-dimensional environments, they remain fundamentally limited by poor sample efficiency due to inefficient exploration, and by the complexity of credit assignment over long horizons.
            In this work, we present Efficient Learning of High-Level Plans from Play (ELF-P), a framework for robotic learning that bridges motion planning and deep RL to achieve long-horizon complex manipulation tasks.
            We leverage task-agnostic <em>play</em>  data to learn a discrete behavioral prior over object-centric primitives, modeling their feasibility given the current context.
            We then design a high-level goal-conditioned policy which (1) uses primitives as building blocks to scaffold complex long-horizon tasks and (2) leverages the behavioral prior to accelerate learning.
            We demonstrate that ELF-P has significantly better sample efficiency than relevant baselines over multiple realistic manipulation tasks and learns policies that can be easily transferred to physical hardware.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<body>


  <section class="section_video">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> Video</h2>
          <div class="publication-video">
              <video controls>
                  <source src="video/Video_Efficient_Learning_High_Level_Plans_from_Play.webm" type="video/webm" />
              </video>
          </div>
        </div>
      </div>
    </div>
  </section>



  <p>
  <h2 class="title is-3">A.1 Environment</h2>
  <h3 class="title is-5">A.1.1 Training environment</h3>

  <p>The environment is built on <em>Pybullet</em> physics simulation
  software [14] and it
  is shown in Figure <a href="#fig:pybullet-env">S1</a>. The states
  <span class="math inline">\(\in \mathbb{R}^{11}\)</span> include the 3D
  robot's end-effector position, a binary variable representing the
  gripper state (open/close), the 3D position of the block and the 1D
  joint position for each of the 3 drawers and the door. The goal state
  <span class="math inline">\(\in \mathbb{R}^3\)</span> is the desired
  block position (e.g. behind the door, inside the mid-drawer or somewhere
  on the table). The action space is discrete and consists of 10
  object-centric motion primitives, namely reaching every object,
  manipulating the objects (e.g. sliding or pulling), and opening and
  closing the gripper. Given the primitives we use are relational to
  objects they become implicitly parameterized by the corresponding object
  pose. We also include a <em>center</em> action to move the robot back to
  the center of the desk to have greater reachability. Finally, we also
  include a <em>go-to-goal</em> primitive that moves the end-effector over
  the goal position. A complete action list is shown below:</p>
  <div class="multicols">
  <ol>
  <li><p>Go to door handle</p></li>
  <li><p>Go to drawer1 handle</p></li>
  <li><p>Go to drawer2 handle</p></li>
  <li><p>Go to drawer3 handle</p></li>
  <li><p>Go to center</p></li>
  <li><p>Go to block</p></li>
  <li><p>Go to goal</p></li>
  <li><p>Grasp/release (Open/close the gripper)</p></li>
  <li><p>Pull/push</p></li>
  <li><p>Slide left/slide right</p></li>
  </ol>
  </div>


  <figure>
    <img  src="figures-appendix/fig1_pybullet.png" alt="image" width="400" height="auto" id="fig:pybullet-env"/></p>
    <figcaption>Figure S1: Training desk environment with different objects that can be manipulated. The robot end-effector is represented with a yellow sphere.</figcaption>
  </figure>

  <p>In the case of multiple primitives for one action (i.e. actions 8, 9
  and 10), the executed primitive depends on the current state (e.g. the
  gripper opens if its current state is closed and viceversa).</p>
  <p>If the agent attempts an infeasible action, such as manipulating an
  object with the wrong primitive (e.g. pulling a sliding door) or moving
  in a non collision-free path, the environment does not perform a
  simulation step and the state remains unaltered. To check for
  collisions, we use the method in <em>Pybullet</em>, which performs a
  single raycast.</p>
  <p>We use the sparse reward signal: <span
  class="math inline">\(R(\cdot|s, g) = ùüô_{\{|f(s)-g|_1 &lt;
  \epsilon\}}\)</span>, where <span class="math inline">\(f(s)\)</span>
  extracts the current block position from the state and <span
  class="math inline">\(\epsilon=0.1\)</span> is a threshold determining
  task success.</p>
  <h2 id="app:experiments" class="title is-3">A.2 Extended Experimental Results</h2>
  <h3 id="her" class="title is-5">A.2.1 HER</h3>
  <p>We experiment with combining our method with off-policy goal
  relabeling [34]. We evaluate different
  relabeling ratios <span class="math inline">\(k\)</span>. As already
  mentioned in the Results, we observe that the greater the relabeling
  ratio, the slower the convergence to the optimal policy (see Figure <a
  href="#fig:her-results">S2</a>). We hypothesize that this is
  because the environment dynamics are not smooth and the policy fails to
  generalize to distant goals despite the relabeling, which may hurt
  performance.</p>
  

  <figure>
  <img src="figures-appendix/legend_her_flat.jpg" alt="image"style="width:50%">
  </figure>

  <!--
  <div class="center">
  <p><embed src="figures-appendix/ours_her_medium.jpg" width="400" height="auto"/> 
  <embed src="figures-appendix/ours_her_hard.jpg" width="400" height="auto" id="fig:her-results"/></p>
  </div>
  <figcaption aria-hidden="false"> Figure S2: Effect of relabeling ratio on success
    rate for the Medium (left) and Hard (right) task variants. Results are
    averaged across 10 independent random seeds, shaded area represents
    standard deviation. (Play dataset size <span
    class="math inline">\(10^5\)</span>). </figcaption> 
  -->



    <div class="row">
      <div class="column2">
        <img  src="figures-appendix/ours_her_medium.jpg" style="width:90%">
      </div>
      <div class="column2">
        <img src="figures-appendix/ours_her_hard.jpg" style="width:90%" id="fig:her-results">
      </div>
    </div>
    <figcaption aria-hidden="false"> Figure S2: Effect of relabeling ratio on success
      rate for the Medium (left) and Hard (right) task variants. Results are
      averaged across 10 independent random seeds, shaded area represents
      standard deviation. (Play dataset size <span
      class="math inline">\(10^5\)</span>). </figcaption> 

  <h3 id="app:sample-complexity" class="title is-5">A.2.2 Sample complexity</h3>
  <p>Results reported in Subsection <em>V-A3) Sample Complexity</em> (main paper) are obtained by
  running 5 independent random seeds for each value of <span
  class="math inline">\(\rho\)</span> in the Medium task. We report
  results obtained for the Hard task in Figure <a
  href="#fig:sample-complxity-hard">S3</a>,
  which shows an increase of sample complexity as the number of feasible
  actions increases. However, in this hard setting, several seeds for
  several values of <span class="math inline">\(\rho\)</span> don‚Äôt reach
  a success rate of 0.95. This is expected given that for example <span
  class="math inline">\(\rho=0\)</span> recovers DDQN behavior which was
  shown to fail in learning the Hard task. Consequently, in order not to
  bias the results, we opt for only reporting the results on <span
  class="math inline">\(\rho\)</span> values whose seeds are always
  successful in learning.</p>
  


  <figure>
    <img src="figures_icra/boxplot_hard_True.jpg" width="400" height="auto"id="fig:sample-complxity-hard"/>
     <figcaption> Figure S3: Effect of <span
       class="math inline">\(\rho\)</span> on sample complexity for the Hard task. Means across seeds are connected by lines.</figcaption>

  </figure>


  <h3 id="app:robustness-datasize" class="title is-5">A.2.3 Robustness to play dataset size</h3>
  <p>We study the effect of play dataset size on training performance for
  both ELF-P and DDQN+Prefill (which affects the amount of data used for
  training the prior for ELF-P and the amount of data used to prefill the replay
  buffer for DDQN+Prefill). In Figure <a
  href="#fig:trainig-curves-datasize-effect">S4</a>
  we show the resulting training curves using 3 different dataset sizes
  for ELF-P and DDQN+Prefill. 
  <!--In Figure <a
  href="#fig:full-datasize-simulation-results" data-reference-type="ref"
  data-reference="fig:full-datasize-simulation-results">2</a> we show the
  resulting training curves for all baselines using the biggest dataset
  (with <span class="math inline">\(10^5\)</span> datapoints).</p>
  -->



  <figure>
    <img src="figures-appendix/legend_datasize_effect.png" alt="image"  style="width:30%">
    </figure>

<div class="row">
  <div class="column2">
    <img src="figures-appendix/medium_datasize_effect.jpg" style="width:90%">
  </div>
  <div class="column2">
    <img src="figures-appendix/medium_inv_action_datasize_effect.jpg" style="width:90%">
  </div>
<div class="row">
  <div class="column2">
    <img src="figures-appendix/hard_datasize_effect.jpg" width="400" style="width:90%">
  </div>
  <div class="column2">
    <img src="figures-appendix/hard_inv_action_datasize_effect.jpg" style="width:90%" id="fig:trainig-curves-datasize-effect">
  </div>
</div>
  <figcaption> Figure S4: Success rate and number of infeasible actions attempts for the Medium
    (top) and Hard (bottom) task variants for 3 different dataset sizes:
    from darker to lighter <span
    class="math inline">10<sup>3</sup>, 10<sup>4</sup> and
    10<sup>5</sup></span> datapoints respectively. When using <span
    class="math inline">10<sup>3</sup></span> datapoints DDQN+Prefill fails
    completely in solving the Hard task. (Results are averaged across 10
    independent random seeds, shaded area represents standard
    deviation)</figcaption>
  </div>
  
  <!--
  <figure>
  <embed src="figures-appendix/legend.pdf"
  id="fig:full-datasize-simulation-results" />
  <figcaption aria-hidden="true">Success rate and number of infeasible
  actions attempts for the Medium (top) and Hard (bottom) task variants
  when using the biggest play dataset (<span
  class="math inline">\(10^5\)</span> datapoints). (Results are averaged
  across 10 independent random seeds, shaded area represents standard
  deviation). </figcaption>
  </figure>
  <div class="center">
  <p><embed src="figures-appendix/medium.pdf" /> <embed
  src="figures-appendix/medium_inv_action.pdf" /></p>
  <p><embed src="figures-appendix/hard.pdf" /> <embed
  src="figures-appendix/hard_inv_action.pdf" /></p>
  </div>
  <p><span id="fig:full-datasize-simulation-results"
  label="fig:full-datasize-simulation-results"></span></p>
  -->
  <h3 id="robustness-to-imperfect-skill-execution" class="title is-5">A.2.4 Robustness to imperfect
  skill execution</h3>

  <p>In this experiment, we evaluate how robust the trained agent is to
    imperfect skill execution. We perform test-time evaluations in which we
    simulate that the execution of the <em>slide, pull/push</em> and
    <em>grasp</em> skills is not successful with some probability. We
    specifically simulate that the execution of <em>slide (pull/push)</em>
    skill fails by setting the door (drawer) joint positions to random
    configurations whereas we simulate that the execution of <em>grasp</em>
    skill is unsuccessful by not updating the grasp state. In Table <a
    href="#table:skill-failure">S1</a>, we
    report average success rate with increasing failure probabilities for ELF-P
    and the most competitive baseline DDQN+Prefill on the Hard task. We
    observe that despite training with perfect skill execution, ELF-P is
    relatively robust to imperfect skill execution and it is able to
    generalize to out of training distribution states, whereas DDQN+Prefill
    is less robust to skill execution failure.</p>

  <div class="table*">
  <table id="table:skill-failure">
  <thead>
  <tr class="header">
  <th style="text-align: center;">Skill failure probability</th>
  <th style="text-align: center;">Success rate ELF-P</th>
  <th style="text-align: center;">Success rate DDQN+Prefill</th>
  <th style="text-align: center;"></th>
  <th style="text-align: center;"></th>
  <th style="text-align: center;"></th>
  <th style="text-align: center;"></th>
  </tr>
  </thead>
  <tbody>
  <tr class="odd">
  <td style="text-align: center;">0</td>
  <td style="text-align: center;">1.0 <span
  class="math inline">\(\pm\)</span> 0.00</td>
  <td style="text-align: center;">1.0 <span
  class="math inline">\(\pm\)</span> 0.00</td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">0.05</td>
  <td style="text-align: center;">0.99 <span
  class="math inline">\(\pm\)</span> 0.01</td>
  <td style="text-align: center;">0.96 <span
  class="math inline">\(\pm\)</span> 0.01</td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">0.1</td>
  <td style="text-align: center;">0.96 <span
  class="math inline">\(\pm\)</span> 0.02</td>
  <td style="text-align: center;">0.92 <span
  class="math inline">\(\pm\)</span> 0.00</td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">0.2</td>
  <td style="text-align: center;">0.91 <span
  class="math inline">\(\pm\)</span> 0.02</td>
  <td style="text-align: center;">0.84 <span
  class="math inline">\(\pm\)</span> 0.03</td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">0.5</td>
  <td style="text-align: center;">0.75 <span
  class="math inline">\(\pm\)</span> 0.03</td>
  <td style="text-align: center;">0.54 <span
  class="math inline">\(\pm\)</span> 0.04</td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  <td style="text-align: center;"></td>
  </tr>
  </tbody>
  <caption>Table S1: Success rate for different skill failure probabilities on the Hard task. Reported results are averaged over 100 evaluation episodes and averaged over 5 random seeds.</caption>
  </table>

  </div>
  


  <h3 id="app:soft-elfp" class="title is-5">A.2.5 Soft prior integration</h3>
  <p>We report results on the (M) task for the Soft prior integration experiment (see Subsection V.A.3 in main paper).
  In Figure <a href="#fig:soft_vs_hard"
  data-reference-type="ref"
  data-reference="fig:soft_vs_hard">[fig:soft_vs_hard]</a> we compare the
  performance of ELF-P with Soft-ELF-P. We observe that while Soft-ELF-P is able to learn
  both medium and hard tasks, it has lower sample efficiency than ELF-P. One of
  the main reasons of ELF-P being faster is because a hard prior integration
  alleviates the Q-network from learning values for all state-action
  pairs: it can focus on learning Q-values for feasible state-action pairs
  only and ignore Q-values for unfeasible actions. This shows one of the
  main contributions of our algorithm. Nevertheless, a soft integration
  could be useful when dealing with degenerated priors and we reserve
  further exploration on the topic for future work.</p>

  <figure>
  <img src="figures-appendix/legend_with_soft.png"  style="width:30%">
  </figure>

  <div class="row">
    <div class="column2">
    <img src="figures-appendix/medium_with_soft_datapoints10000.jpg" style="width:90%">
  </div>
    <div class="column2">
    <img src="figures-appendix/medium_inv_action_with_soft_datapoints10000.jpg" style="width:90%"> id="fig:soft_vs_hard_appendix"/></p>
  </div>
</div>
  <figcaption aria-hidden="true">Comparison between ELF-P and Soft-ELF-P. Success
    rate and number of infeasible actions attempts for the Medium task variant. (Play dataset size <span
    class="math inline">\(10^4\)</span>).Results are averaged across 10
    independent random seeds, shaded area represents standard deviation)
    </figcaption>


  <h2 id="app:additional_exp_details" class="title is-3">A.3 Additional experimental details</h2>
  <h3 id="evaluation-protocol" class="title is-5">A.3.1 Evaluation protocol</h3>
  <p>All reported results are obtained by evaluating the policies over 50
  episodes every 2500 environment steps. Results are averaged over 10
  random independent seeds unless explicitly stated. Shaded areas
  represent standard deviation across seeds.</p>
  <h3 id="play-dataset-collection" class="title is-5">A.3.2 <em>Play</em>-dataset collection</h3>
  <p>Given that our method can learn with very little data (<span
  class="math inline">\(\sim\)</span>1h30min of data collection), play
  data could in practice be collected by a few human operators choosing
  from a predefined set of primitives. However for simplicity, we resort
  to evaluating whether a termination condition for each primitive is met
  in every configuration. This is a common approach under the options
  framework [30].
  The tuples <span class="math inline">\((s,a)\)</span>, containing the
  state of the environment and the feasible action performed in the state
  respectively, are stored in the <em>play</em>-dataset <span
  class="math inline">\(\mathcal{D}\)</span> for training the prior.</p>
  
  <h3 id="app:baselines" class="title is-5">A.3.3 Hyperparameters and Architectures</h3>
  <p>Across all baselines and experiments, unless explicitly stated, we
  use the same set of hyperparameters and neural network architectures
  Values are reported in Table <a href="#table:hyperparams-new">Table S2</a>. We
  choose a set of values that was found to perform well in previous works.
  We report the list of hyperparameters that were tuned for each method in
  the following subsections.</p>


<!---->


<div class="table*">
<div class="adjustbox">
<table id="table:hyperparams-new">
  <thead>
  <tr class="header">
  <th style="text-align: center;"><strong>Parameter</strong></th>
  <th style="text-align: center;"><strong>Value</strong></th>
  <th style="text-align: center;"></th>
  </tr>
  </thead>
  <tbody>
  <tr class="odd">
  <td style="text-align: center;">Q-network architecture</td>
  <td style="text-align: center;">MLP<span class="math inline"> \( [128, 256]\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">Batch size</td>
  <td style="text-align: center;"><span
    class="math inline">\(256\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">Exploration technique</td>
  <td style="text-align: center;"><span
  class="math inline">\(\epsilon\)</span>-greedy with exponential
  decay</td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">Initial <span
  class="math inline">\(\epsilon\)</span></td>
  <td style="text-align: center;"><span
    class="math inline">\(0.5\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">Decay rate for <span
  class="math inline">\(\epsilon\)</span></td>
  <td style="text-align: center;"><span
    class="math inline">\(5e^{-5}\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">Discount <span
  class="math inline">\(\gamma\)</span></td>
  <td style="text-align: center;"><span
    class="math inline">\(0.97\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">Optimizer</td>
  <td style="text-align: center;">Adam (<span
  class="math inline">\(\beta_1=0.9, \beta_2=0.999\)</span>) [15]</td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">Learning rate <span
  class="math inline">\(\eta\)</span></td>
  <td style="text-align: center;">1e-4</td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">Episode length <span
  class="math inline">\(T\)</span></td>
  <td style="text-align: center;"><span
    class="math inline">\(100\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">Experience replay size</td>
  <td style="text-align: center;"><span
    class="math inline">\(1e^{6}\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">Initial exploration steps</td>
  <td style="text-align: center;"><span
    class="math inline">\(2000\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">Steps before training starts</td>
  <td style="text-align: center;"><span
    class="math inline">\(1000\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">Steps between parameter updates</td>
  <td style="text-align: center;"><span
    class="math inline">\(50\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="even">
  <td style="text-align: center;">Soft target update parameter <span
  class="math inline">\(\mu\)</span></td>
  <td style="text-align: center;"><span
    class="math inline">\(0.995\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  <tr class="odd">
  <td style="text-align: center;">Threshold <span
  class="math inline">\(\rho\)</span></td>
  <td style="text-align: center;"><span
    class="math inline">\(0.01\)</span></td>
  <td style="text-align: center;"></td>
  </tr>
  </tbody>
  <caption>Table S2: Architecture parameters and hyperparameters used for all the baselines and ELF-P.</caption>
  </table>
  </div>
  </div>





  <div class="table*">
    <div class="adjustbox">
      <table id="table:hyperparams-prior">
        <thead>
    <tr class="header">
    <th style="text-align: center;"><strong>Parameter</strong></th>
    <th style="text-align: center;"><strong>Value</strong></th>
    <th style="text-align: center;"></th>
    </tr>
    </thead>
    <tbody>
    <tr class="odd">
    <td style="text-align: center;">Prior architecture</td>
    <td style="text-align: center;">MLP <span
      class="math inline">\([200,200]\)</span></td>
    <td style="text-align: center;"></td>
    </tr>
    <tr class="even">
    <td style="text-align: center;">Prior batch size</td>
    <td style="text-align: center;"><span
      class="math inline">\(500\)</span></td>
    <td style="text-align: center;"></td>
    </tr>
    <tr class="odd">
    <td style="text-align: center;">Prior training steps</td>
    <td style="text-align: center;"><span
      class="math inline">\(1e^{-5}\)</span></td>
    <td style="text-align: center;"></td>
    </tr>
    <tr class="even">
    <td style="text-align: center;">Dataset size</td>
    <td style="text-align: center;"><span
      class="math inline">\(10000\)</span></td>
    <td style="text-align: center;"></td>
    </tr>
    <tr class="odd">
    <td style="text-align: center;">Prior optimizer</td>
    <td style="text-align: center;">Adam(<span
    class="math inline">\(\beta_1=0.9, \beta_2=0.999\)</span> [15]</td>
    <td style="text-align: center;"></td>
    </tr>
    <tr class="even">
    <td style="text-align: center;">Learning rate</td>
    <td style="text-align: center;"><span
      class="math inline">\(1e^{-3}\)</span></td>
    <td style="text-align: center;"></td>
    </tr>
    </tbody>
    <caption> Table S3: Architecture parameters and hyperparameters used for training the prior in ELF-P.</caption>
    </table>
    </div>
    </div>











  
  <strong>DDQN</strong>
  <ul>
  <li><p>Discount <span class="math inline">\(\gamma\)</span>: <span
  class="math inline">\(0.97\)</span>. Tuned over <span
  class="math inline">\([0.90, 0.95, 0.97, 0.99]\)</span>.</p></li>
  </ul>
  <strong>DDQN+HER</strong>
  <ul>
  <li><p>Relabeling ratio <span class="math inline">\(k\)</span>: <span
  class="math inline">\(4\)</span> (as suggested in the original paper
 [34]).</p></li>
  <li><p>Discount <span class="math inline">\(\gamma\)</span>: <span
  class="math inline">\(0.97\)</span>. Tuned over <span
  class="math inline">\([0.90, 0.95, 0.97, 0.99]\)</span>.</p></li>
  </ul>
  <strong>DDQN+Prefill</strong>
  <ul>
  <li><p>Discount <span class="math inline">\(\gamma\)</span>: <span
  class="math inline">\(0.95\)</span>. Tuned over <span
  class="math inline">\([0.90, 0.95, 0.97, 0.99]\)</span>.</p></li>
  <li><p>Prefill dataset: The dataset used to prefill the replay buffer is
  the same as the <em>Play</em>-dataset used to train the behavioral
  prior, but extending the tuples <span class="math inline">\((s,a) \to
  (s,a,s&#39;,g,r)\)</span> to be able to perform TD-loss on them. Given
  that the <em>Play</em>-dataset is task-agnostic, we decide to compute
  the rewards with relabelling, i.e., to relabel the <span
  class="math inline">\(g\)</span> to the achieved goal and to sgoal et the
  reward <span class="math inline">\(r\)</span> to <span
  class="math inline">\(1\)</span>, otherwise set the reward to zero. We
  experiment with different relabeling frequencies <span
  class="math inline">\(f \in [0.0, 0.25, 0.5, 0.75, 1.0]\)</span>. We
  find that <span class="math inline">\(f&gt;0\)</span> leads to
  overestimation of the Q-values at early stages of training and thus hurt
  performance. For this reason we use <span
  class="math inline">\(f=0\)</span>, i.e., no relabelling.</p></li>
  </ul>
  <strong>DQfD</strong>
  <ul>
  <li><p>Discount <span class="math inline">\(\gamma\)</span>: <span
  class="math inline">\(0.95\)</span>. Tuned over <span
  class="math inline">\([0.90, 0.95, 0.97, 0.99]\)</span>.</p></li>
  <li><p>Large margin classification loss weight <span
  class="math inline">\(\lambda_2\)</span>: <span
  class="math inline">\(1e^{-3}\)</span>. Tuned over <span
  class="math inline">\([1e^{-2}, 1e^{-3}, 1e^{-4},
  1e^{-5}]\)</span>.</p></li>
  <li><p>Expert margin: <span class="math inline">\(0.05\)</span>. Tuned
  over <span class="math inline">\([0.01, 0.05, 0.1, 0.5,
  0.8]\)</span>.</p></li>
  <li><p>L2 regularization loss weight <span
  class="math inline">\(\lambda_3\)</span>: <span
  class="math inline">\(1e^{-5}\)</span>.</p></li>
  <li><p>Prefill dataset: Same approach as for DDQN+Prefill explained
  above.</p></li>
  </ul>
  <strong>SOFT ELF-P</strong>
  <ul>
  <li><p>Discount <span class="math inline">\(\gamma\): \(0.97\)</span>. Tuned over <span
  class="math inline">\([0.90, 0.95, 0.97, 0.99]\)</span>.</p></li>
  </ul>
  <strong>SPiRL</strong>
  <ul>
  <li><p>Discount <span class="math inline">\(\gamma\): \(0.97\)</span>. Tuned over <span
  class="math inline">\([0.90, 0.95, 0.97, 0.99]\)</span>.</p></li>
  <li><p>KL weight <span class="math inline">\(\alpha\): \(0.01\)</span>
  Tuned over <span class="math inline">\([0.005, 0.01,
  0.05]\)</span>.</p></li>
  <li><p>Actor-network architecture: MLP<span class="math inline"> \( [128, 256]\)</span></p></li>
  </ul>
  <strong>ELF-P</strong>
  <ul>
  <li><p>Discount <span class="math inline">\(\gamma\)</span>: <span
  class="math inline">\(0.97\)</span>. Tuned over <span
  class="math inline">\([0.90, 0.95, 0.97, 0.99]\)</span>.</p></li>
  <li><p>Prior training: we use the parameters reported in Table <a
  href="#table:hyperparams-prior">S3</a>.</p></li>
  </ul>
  
  
  
  <h2 id="app:hardware_exp" class="title is-3">A.4 Hardware experiments</h2>
  <p>The experiments are carried out on a real desktop setting depicted in
  Figure 1 (main paper)(upper right). We use
  <em>Boston Dynamics</em> Spot robot [56] for all our experiments. The
  high-level planner, trained in simulation, is used at inference time to
  predict the required sequence of motion primitives to achieve a desired
  goal. The motion primitives are executed using established motion
  planning methods [73] to create optimal control
  trajectories. We then send desired commands to Spot using <em>Boston
  Dynamics Spot SDK</em>, a Python API provided by Boston Dynamics, which
  we interface with using <em>pybind11</em>. We refer the reader to the
  Video material for a visualization of the real-word experiments.</p>
  <h2 id="app:optimality"class="title is-3">A.5 Optimality</h2>
  <p>Let us consider an MDP <span
  class="math inline">\(\mathcal{M}\)</span>, the reduced MDP <span
  class="math inline">\(\mathcal{M&#39;}\)</span> as defined in Subsection
  <em>IV-C) Learning in a Reduced MDP</em> (main paper), the
  selection operator <span class="math inline">\({\alpha: \mathcal{S \to
  P(A)}}\)</span> and a set of feasible policies under <span
  class="math inline">\(\alpha\)</span>: <span
  class="math inline">\(\Pi_\alpha=\{ \pi | \pi(s,g) \in \alpha(s) \forall
  s \in \mathcal{S}, g \in \mathcal{G} \}\)</span>.</p>
  <div class="theorem">
  <p>Given an MDP <span class="math inline">\(\mathcal{M}\)</span>, the
  reduced MDP <span class="math inline">\(\mathcal{M&#39;}\)</span> and a
  selection operator <span class="math inline">\(\alpha\)</span> such that
  the optimal policy for <span class="math inline">\(\mathcal{M}\)</span>
  belongs to <span class="math inline">\(\Pi_\alpha\)</span> (that is
  <span class="math inline">\(\pi_{\mathcal{M}}^*(s, g) \in
  \Pi_\alpha\)</span>), the optimal policy in <span
  class="math inline">\(\mathcal{M&#39;}\)</span> is also optimal for
  <span class="math inline">\(\mathcal{M}\)</span>, i.e. <span
  class="math inline">\(\pi_{\mathcal{M&#39;}}^*(s, g) =
  \pi_{\mathcal{M}}^*(s, g)\)</span>.</p>
  </div>
  <div class="proof">
  <p><em>Proof.</em> We can define a goal-conditioned value function <span
  class="math inline">\(V^\pi(s,g)\)</span>, defined as the expected sum
  of discounted future rewards if the agent starts in <span
  class="math inline">\(s\)</span> and follows policy <span
  class="math inline">\(\pi\)</span> thereafter: <span
  class="math inline">\({V^\pi(s, g) =
  \mathbb{E}_{\mu^\pi}\left[\sum_{t-1}^\infty \gamma^{t-1} R(s, g)
  \right]}\)</span> under the trajectory distribution <span
  class="math inline">\({\mu^\pi(\tau|g) = \rho_0(s_0) \prod_{t=0}^\infty
  P(s_{t+1}|s_t, a_t)}\)</span> with <span class="math inline">\(a_t =
  \pi(s_t, g) \; \forall t\)</span>. By construction, the transition
  kernel <span class="math inline">\(P\)</span> and the reward function
  <span class="math inline">\(R\)</span> are shared across <span
  class="math inline">\(\mathcal{M}\)</span> and <span
  class="math inline">\(\mathcal{M&#39;}\)</span>, and a feasible policy
  <span class="math inline">\(\pi\)</span> always selects the same action
  in both MDPs, thus trajectory distributions and value functions are also
  identical for feasible policies. More formally, if <span
  class="math inline">\(\pi \in \Pi_\alpha\)</span>, then <span
  class="math inline">\(\mu^\pi_{\mathcal{M}}=\mu^\pi_{\mathcal{M&#39;}}\)</span>
  and <span class="math inline">\(V^\pi_{\mathcal{M}}(s, g) =
  V^\pi_{\mathcal{M&#39;}}(s, g)\)</span>.</p>
  <p>It is then sufficient to note that <span
  class="math display">\[\pi_{\mathcal{M&#39;}}^*(s,
  g)\stackrel{def}{=}\arg\max_{\pi \in \Pi_\alpha}
  V^{\pi}_{\mathcal{M&#39;}}(s, g)=\arg\max_{\pi \in \Pi_\alpha}
  V^{\pi}_{\mathcal{M}}(s,g)=\arg\max_{\pi \in \Pi}
  V^{\pi}_{\mathcal{M}}(s, g)\stackrel{def}{=}\pi_{\mathcal{M}}^*(s,
  g),\]</span> where the second equality is due to the previous statement,
  and the third equality is granted by the assumption that the optimal
  policy for <span class="math inline">\(\mathcal{M}\)</span> is feasible
  in <span class="math inline">\(\mathcal{M&#39;}\)</span> (i.e. <span
  class="math inline">\(\pi_\mathcal{M}^* \in \Pi_\alpha\)</span>).‚óª</p>
  </div>
  <p>Hence, our proposed algorithm, which allows us learning directly on
  the reduced MDP <span class="math inline">\(\mathcal{M&#39;}\)</span>,
  can under mild assumptions retrieve the optimal policy in the original
  <span class="math inline">\(\mathcal{M}\)</span>.</p>
  <p>We finally remark that model-free PAC-MDP algorithms have shown to
  produce upper bounds on sample complexity that are <span
  class="math inline">\(\tilde O(N)\)</span>, where <span
  class="math inline">\(N \leq |\mathcal{S}||\mathcal{A}|\)</span>[64], i.e., directly
  dependent on the number of state-action pairs. Hence, learning in the
  reduced MDP <span class="math inline">\(\mathcal{M&#39;}\)</span>
  instead of <span class="math inline">\(\mathcal{M}\)</span> could lead
  to near-linear improvements in sample efficiency as the number of
  infeasible actions grows. We demonstrate it empirically in the
  Subsection <em>V-A3) Sample Complexity</em> (main paper) and <a
  href="#app:sample-complexity"> A.2.2 Sample Complexity</a> .</p>
  </body>
  </html>
  


